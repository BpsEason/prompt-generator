# app/utils/llm_connector.py - LLM API ä¸²æ¥å±¤ (ç•°æ­¥åŒ–)

from typing import Dict, Any
import asyncio
import random
import json

# å‡è¨­é€™è£¡ä½¿ç”¨ OpenAI Python SDK çš„éåŒæ­¥å®¢æˆ¶ç«¯
# import openai
# from openai import AsyncOpenAI
# client = AsyncOpenAI(api_key=...)

async def call_llm_api(prompt: str, model: str, task: str = "generation") -> str:
    """
    çµ±ä¸€çš„ LLM API ç•°æ­¥å‘¼å«å‡½æ•¸ (ä½¿ç”¨ asyncio.sleep æ¨¡æ“¬å»¶é²)ã€‚
    """
    
    print(f"\n--- æ¨¡æ“¬ LLM å‘¼å«: Model={model}, Task={task} ---")
    
    # æ¨¡æ“¬ API å‘¼å«çš„ç¶²è·¯å»¶é²ï¼Œé‡‹æ”¾ Event Loop
    await asyncio.sleep(random.uniform(0.1, 0.5)) 
    
    if task == "rubric_check":
        # æ¨¡æ“¬ Rubric Checker ä»»å‹™çš„ JSON è¼¸å‡º
        mock_result = {
            "score_target_audience": random.randint(3, 5),
            "score_brand_style": random.randint(3, 5),
            "score_cta_clarity": random.randint(3, 5),
            "score_feasibility": random.randint(3, 5),
            "notes": f"æ¨¡æ“¬ Rubric è©•åˆ†çµæœ (Model: {model})ã€‚è«‹æ›¿æ›ç‚ºçœŸå¯¦ LLM è¼¸å‡ºã€‚"
        }
        # Rubric Checker æ‡‰è¿”å› JSON å­—ä¸²
        return json.dumps(mock_result, ensure_ascii=False, indent=2)
    
    elif task == "generation":
        # æ¨¡æ“¬è¡ŒéŠ·æ–‡æ¡ˆç”Ÿæˆä»»å‹™
        style = "ç†±è¡€" if "ç†±æƒ…" in prompt else "å°ˆæ¥­"
        product = "AI ç°¡å ±å·¥å…·"
        cta = "ç«‹å³å…è²»è©¦ç”¨" if "ç«‹å³å…è²»è©¦ç”¨" in prompt else "æ­¡è¿è¯ç¹«"
        
        return f"""
        ã€{style}è¡åˆºï¼ç•°æ­¥è™•ç†è®“ä½ çš„å°ˆæ¡ˆæ›´é«˜æ•ˆï¼ã€‘
        å‰›ç•¢æ¥­çš„ä½ ï¼Œæ˜¯å¦å°ç°¡å ±æ„Ÿåˆ°ç„¦æ…®ï¼Ÿåˆ¥æ“”å¿ƒï¼{product} è®“ä½ ä¸€éµç”Ÿæˆå°ˆæ¥­ç°¡å ±ï¼Œ
        æŠŠæ™‚é–“ç”¨åœ¨æœ€é‡è¦çš„äº‹ä¸Šï¼å……åˆ†åˆ©ç”¨äº† FastAPI çš„ç•°æ­¥ç‰¹æ€§ï¼Œæé«˜ååé‡ï¼
        #ç•°æ­¥ #ç”Ÿç”¢ç´š #AIå·¥å…·
        **ğŸ”¥ è¶•å¿«è¡Œå‹•ï¼{cta}ï¼**
        """
    
    return f"Mock Output for prompt: {prompt[:50]}..."
